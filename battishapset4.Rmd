---
title: "Battisha PSet 3"
output: pdf_document
---


```{r}
library(tidyverse)
library(dplyr)
library(caret)
library(rsample)
library(ggplot2)
library(seriation)
library(skimr)
library(dendextend)
library(mixtools)
library(plotGMM)
library(factoextra)
library(clValid)
library(raster)
```

### Performing k-Means By Hand

##Q1: Plot the Observations
```{r}
#Generate Dataset
sim_data <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(sim_data, main="Plotting of Simulated Dataset", xlab="Column 1 Values", ylab="Column 2 Values")
```

#Q2: Set the Clusters
```{r}
set.seed(27)
classified<-cbind(sim_data, sample(1:2, 6, replace=T))
plot(classified[,1], classified[,2], col=classified[,3], main="Plotting of Simulated Dataset (Clustered)", xlab="Column 1 Values", ylab="Column 2 Values")
```

##Q3: Compute the Centroids
```{r}
#Find Centroid 1
centroid.1 <- cbind(mean(classified[classified[,3]==1,][,1]), mean(classified[classified[,3]==1,][,2]))

#Find Centroid 2
centroid.2 <- cbind(mean(classified[classified[,3]==2,][,1]), mean(classified[classified[,3]==2,][,2]))

centroid.1
centroid.2
```

##Q4: Assign each observation to the centroid to which it is closest, in terms of Euclidean distance.
```{r}
for (i in 1:6){
  if (pointDistance(classified[i,1:2], centroid.1, lonlat=FALSE) > pointDistance(classified[i,1:2], centroid.2, lonlat=FALSE)){
    classified[i,3] <- 2
  }
  else {
    classified[i,3] <- 1
  }
}
classified
```


##Q5: Keep running until results stablize
```{r}
#Find Centroid 1
centroid.1 <- cbind(mean(classified[classified[,3]==1,][,1]), mean(classified[classified[,3]==1,][,2]))

#Find Centroid 2
centroid.2 <- cbind(mean(classified[classified[,3]==2,][,1]), mean(classified[classified[,3]==2,][,2]))

for (i in 1:6){
  if (pointDistance(classified[i,1:2], centroid.1, lonlat=FALSE) > pointDistance(classified[i,1:2], centroid.2, lonlat=FALSE)){
    classified[i,3] <- 2
  }
  else {
    classified[i,3] <- 1
  }
}

(finalturn <- classified)
```

##Q6: Plot Results
```{r}
#Original
set.seed(27)
classified<-cbind(sim_data, sample(1:2, 6, replace=T))
plot(classified[,1], classified[,2], col=classified[,3], main="Plotting of Simulated Dataset (Inital)", xlab="Column 1 Values", ylab="Column 2 Values")

#After Stabilizing (only took one turn)
plot(finalturn[,1], finalturn[,2], col=finalturn[,3], main="Plotting of Simulated Dataset (Final--only one turn required)", xlab="Column 1 Values", ylab="Column 2 Values")

```


### Clustering State Legislative Professionalism

##Q1
```{r}
#load data
leg <- get(load("Data and Codebook/legprof-components.v1.0.RData"))
leg
```


##Q2: Data Munging
```{r}
selected <- leg %>%
  #Select 2009/10 session
  filter(sessid == "2009/10") %>%
  #Select only continous features
  dplyr::select(state, t_slength, slength, salary_real, expend) %>%
  #drop nans
  drop_na()

#save state variable separately
states <- selected$state


changed<-selected %>%
  #remove state variable
  dplyr::select(-state)%>%
  #standardize values
  scale() 
  
  
munged<- changed%>%
  #Matrix
  dist()
```


##Q3: Diagnose Clusterability
```{r}
dissplot(munged, method="VAT")
dissplot(munged, method="ARSA")
dissplot(munged, method="Spectral")
```
I opted for an ODI using the dissplot function, but utilized multiple methods to see different categorizations presented themselves. The clearest of the ODIs in my opinion was generated by the was the VAT method, which presented 6 rough categories. Importantly, however, in all three ODIs, we can see that there is one distinct square towards the top corner or the bottom corner of the chart--if we use only this square as the distinct category from the rest, we get 2 categories. 

##Q4: Agglomerative Hierarchical
```{r}

hc_single <- hclust(munged, 
                    method = "single"); plot(hc_single, labels=states, hang = -1)

hc_complete <- hclust(munged,
                      method = "complete"); plot(hc_complete, labels=states, hang = -1)

hc_average <- hclust(munged, 
                     method = "average"); plot(hc_average, labels=states, hang = -1)

hc_centroid <- hclust(munged,
                      method = "centroid"); plot(hc_centroid, labels=states, hang = -1)


```
I used HAC with four different methods--the one that I found most insightful was the complete method. Here we can see two main categories--one with CA, MA, NY, PA, OH, IL and MI and the other with the rest. These states tend to be blue, and generally have higher industrialization and education than the rest of the nation. 

##Q5: K-Means
```{r}
set.seed(777)

kmeans <- kmeans(munged, 
                 centers = 2,
                 nstart = 2)
```


```{r}
# Inspect the kmeans object
str(kmeans)

# Assess a little more descriptively
t <- as.table(kmeans$cluster)
t <- data.frame(t)
t$state <- states
colnames(t)[colnames(t)=="Freq"] <- "Assignment"
t$Var1 <- NULL

t
sum(t$Assignment==1)
```
K-Means came out with exactly the same clustering as HAC-complete from the last section. Once again, those 6 states have been separated from the rest of the country. 

##Q6: GMM
```{r}
set.seed(380)

gmm <- normalmixEM(changed, k = 2)
ggplot(data.frame(x = gmm$x)) +
  geom_histogram(aes(x, ..density..), fill = "darkgray", bins=50) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm$mu[1], gmm$sigma[1], lam = gmm$lambda[1]),
                colour = "darkgreen") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm$mu[2], gmm$sigma[2], lam = gmm$lambda[2]),
                colour = "red")+
  xlab("State Legislature Efficiency") +
  ylab("Density") + 
  theme_bw()

```
Like the other two methods, the GMM also selected the same range of 6 states. However here, it is interesting to see how far away some of these states are from the rest; the GMM shows how disparate some of the features are in these states from the country as a whole.


#Q7: Compare Outputs
```{r}
#HAC
plot(hc_complete, labels=states, hang = -1)

#K-Means
t

#GMM
ggplot(data.frame(x = gmm$x)) +
  geom_histogram(aes(x, ..density..), fill = "darkgray", bins=50) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm$mu[1], gmm$sigma[1], lam = gmm$lambda[1]),
                colour = "darkgreen") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm$mu[2], gmm$sigma[2], lam = gmm$lambda[2]),
                colour = "red")+
  xlab("State Legislature Efficiency") +
  ylab("Density") + 
  theme_bw()

#Compare Salary and Expenditures based on K-Means Grouping
comparision_table <-merge(x=t,y=selected,by="state")
comp_1 <- comparision_table %>%
  filter(Assignment==1)
comp_2 <- comparision_table %>%
  filter(Assignment==2)
boxplot(comp_1$salary_real, comp_2$salary_real,
        main = "Salary Comparision",
        names = c("Group 1", "Group 2"))
boxplot(comp_1$expend, comp_2$expend,
        main = "Expenditures Comparision",
        names = c("Group 1", "Group 2"))
  
```
As we can see, the HAC, K-Means and GMM all came out with relatively similar results--all three of them selecting the same 6 states (CA, MA, NY, PA, OH, IL and MI) to separate from the rest. The box plots allow us to see a bit about why these states were so distinct from the rest. We can see that in terms of salary, these states pay much more, with the ranges almost not overallapping at all. We can also see that on average, these states expend quite a bit more and in a greater range than the other states.

#Q8: validation:
```{r}
validation <- clValid(changed, c(2:5),
                      clMethods = c("hierarchical", "kmeans",  "pam" ),
                      validation = c("internal", "stability"))
summary(validation)
```

#Question 9:
I decided to focus on the Silhoutte validation metric. According to my ClValid "Optimal Scores" section, my most optimal score for Silhoutte was the 2 clustered Hierarchial, with a score of 0.6994. Thus, under this metric we can see that the optimal approach is hierarchial at k=2. However, it is important to note the other metrics that are presented by my validation algorithm. The optimal methods and clusters for those metrics vary, and we can see that in general using 2 or 5 clusters is best, and hierarchial or kmeans is best as a method. One important reason that I could use the sub-optimal GMM for clustering even when it doesn't have great validation statistics is beause of its nature as a probablistic clusterer. Unlike K-Means and HAC, which don't allow me to see as clearly which entries were most staunchly in a cluster and which were further away, the probabilites in GMM allow me to do so, which can be very useful in some scenarios.
